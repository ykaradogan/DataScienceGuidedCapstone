#!/usr/bin/env python
# coding: utf-8

# # 2 Data wrangling<a id='2_Data_wrangling'></a>

# ## 2.1 Contents<a id='2.1_Contents'></a>
# * [2 Data wrangling](#2_Data_wrangling)
#   * [2.1 Contents](#2.1_Contents)
#   * [2.2 Introduction](#2.2_Introduction)
#     * [2.2.1 Recap Of Data Science Problem](#2.2.1_Recap_Of_Data_Science_Problem)
#     * [2.2.2 Introduction To Notebook](#2.2.2_Introduction_To_Notebook)
#   * [2.3 Imports](#2.3_Imports)
#   * [2.4 Objectives](#2.4_Objectives)
#   * [2.5 Load The Ski Resort Data](#2.5_Load_The_Ski_Resort_Data)
#   * [2.6 Explore The Data](#2.6_Explore_The_Data)
#     * [2.6.1 Find Your Resort Of Interest](#2.6.1_Find_Your_Resort_Of_Interest)
#     * [2.6.2 Number Of Missing Values By Column](#2.6.2_Number_Of_Missing_Values_By_Column)
#     * [2.6.3 Categorical Features](#2.6.3_Categorical_Features)
#       * [2.6.3.1 Unique Resort Names](#2.6.3.1_Unique_Resort_Names)
#       * [2.6.3.2 Region And State](#2.6.3.2_Region_And_State)
#       * [2.6.3.3 Number of distinct regions and states](#2.6.3.3_Number_of_distinct_regions_and_states)
#       * [2.6.3.4 Distribution Of Resorts By Region And State](#2.6.3.4_Distribution_Of_Resorts_By_Region_And_State)
#       * [2.6.3.5 Distribution Of Ticket Price By State](#2.6.3.5_Distribution_Of_Ticket_Price_By_State)
#         * [2.6.3.5.1 Average weekend and weekday price by state](#2.6.3.5.1_Average_weekend_and_weekday_price_by_state)
#         * [2.6.3.5.2 Distribution of weekday and weekend price by state](#2.6.3.5.2_Distribution_of_weekday_and_weekend_price_by_state)
#     * [2.6.4 Numeric Features](#2.6.4_Numeric_Features)
#       * [2.6.4.1 Numeric data summary](#2.6.4.1_Numeric_data_summary)
#       * [2.6.4.2 Distributions Of Feature Values](#2.6.4.2_Distributions_Of_Feature_Values)
#         * [2.6.4.2.1 SkiableTerrain_ac](#2.6.4.2.1_SkiableTerrain_ac)
#         * [2.6.4.2.2 Snow Making_ac](#2.6.4.2.2_Snow_Making_ac)
#         * [2.6.4.2.3 fastEight](#2.6.4.2.3_fastEight)
#         * [2.6.4.2.4 fastSixes and Trams](#2.6.4.2.4_fastSixes_and_Trams)
#   * [2.7 Derive State-wide Summary Statistics For Our Market Segment](#2.7_Derive_State-wide_Summary_Statistics_For_Our_Market_Segment)
#   * [2.8 Drop Rows With No Price Data](#2.8_Drop_Rows_With_No_Price_Data)
#   * [2.9 Review distributions](#2.9_Review_distributions)
#   * [2.10 Population data](#2.10_Population_data)
#   * [2.11 Target Feature](#2.11_Target_Feature)
#     * [2.11.1 Number Of Missing Values By Row - Resort](#2.11.1_Number_Of_Missing_Values_By_Row_-_Resort)
#   * [2.12 Save data](#2.12_Save_data)
#   * [2.13 Summary](#2.13_Summary)
# 

# ## 2.2 Introduction<a id='2.2_Introduction'></a>

# This step focuses on collecting your data, organizing it, and making sure it's well defined. Paying attention to these tasks will pay off greatly later on. Some data cleaning can be done at this stage, but it's important not to be overzealous in your cleaning before you've explored the data to better understand it.

# ### 2.2.1 Recap Of Data Science Problem<a id='2.2.1_Recap_Of_Data_Science_Problem'></a>

# The purpose of this data science project is to come up with a pricing model for ski resort tickets in our market segment. Big Mountain suspects it may not be maximizing its returns, relative to its position in the market. It also does not have a strong sense of what facilities matter most to visitors, particularly which ones they're most likely to pay more for. This project aims to build a predictive model for ticket price based on a number of facilities, or properties, boasted by resorts (*at the resorts).* 
# This model will be used to provide guidance for Big Mountain's pricing and future facility investment plans.

# ### 2.2.2 Introduction To Notebook<a id='2.2.2_Introduction_To_Notebook'></a>

# Notebooks grow organically as we explore our data. If you used paper notebooks, you could discover a mistake and cross out or revise some earlier work. Later work may give you a reason to revisit earlier work and explore it further. The great thing about Jupyter notebooks is that you can edit, add, and move cells around without needing to cross out figures or scrawl in the margin. However, this means you can lose track of your changes easily. If you worked in a regulated environment, the company may have a a policy of always dating entries and clearly crossing out any mistakes, with your initials and the date.
# 
# **Best practice here is to commit your changes using a version control system such as Git.** Try to get into the habit of adding and committing your files to the Git repository you're working in after you save them. You're are working in a Git repository, right? If you make a significant change, save the notebook and commit it to Git. In fact, if you're about to make a significant change, it's a good idea to commit before as well. Then if the change is a mess, you've got the previous version to go back to.
# 
# **Another best practice with notebooks is to try to keep them organized with helpful headings and comments.** Not only can a good structure, but associated headings help you keep track of what you've done and your current focus. Anyone reading your notebook will have a much easier time following the flow of work. Remember, that 'anyone' will most likely be you. Be kind to future you!
# 
# In this notebook, note how we try to use well structured, helpful headings that frequently are self-explanatory, and we make a brief note after any results to highlight key takeaways. This is an immense help to anyone reading your notebook and it will greatly help you when you come to summarise your findings. **Top tip: jot down key findings in a final summary at the end of the notebook as they arise. You can tidy this up later.** This is a great way to ensure important results don't get lost in the middle of your notebooks.

# In this, and subsequent notebooks, there are coding tasks marked with `#Code task n#` with code to complete. The `___` will guide you to where you need to insert code.

# ## 2.3 Imports<a id='2.3_Imports'></a>

# Placing your imports all together at the start of your notebook means you only need to consult one place to check your notebook's dependencies. By all means import something 'in situ' later on when you're experimenting, but if the imported dependency ends up being kept, you should subsequently move the import statement here with the rest.

# In[4]:


#Code task 1#
#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os


# ## 2.4 Objectives<a id='2.4_Objectives'></a>

# There are some fundamental questions to resolve in this notebook before you move on.
# 
# * Do you think you may have the data you need to tackle the desired question?
#     * Have you identified the required target value?
#     * Do you have potentially useful features?
# * Do you have any fundamental issues with the data?

# ## 2.5 Load The Ski Resort Data<a id='2.5_Load_The_Ski_Resort_Data'></a>

# In[ ]:


# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv("ski_resort_data.csv")


# Good first steps in auditing the data are the info method and displaying the first few records with head.

# In[ ]:


#Code task 2#
#Call the info method on ski_data to see a summary of the data
ski_data.___


# `AdultWeekday` is the price of an adult weekday ticket. `AdultWeekend` is the price of an adult weekend ticket. The other columns are potential features.

# This immediately raises the question of what quantity will you want to model? You know you want to model the ticket price, but you realise there are two kinds of ticket price!

# In[ ]:


#Code task 3#
#Call the head method on ski_data to print the first several rows of the data
ski_data.___


# The output above suggests you've made a good start getting the ski resort data organized. You have plausible column headings. You can already see you have a missing value in the `fastEight` column

# ## 2.6 Explore The Data<a id='2.6_Explore_The_Data'></a>

# ### 2.6.1 Find Your Resort Of Interest<a id='2.6.1_Find_Your_Resort_Of_Interest'></a>

# Your resort of interest is called Big Mountain Resort. Check it's in the data:

# In[ ]:


#Code task 4#
#Filter the ski_data dataframe to display just the row for our resort with the name 'Big Mountain Resort'
#Hint: you will find that the transpose of the row will give a nicer output. DataFrame's do have a
#transpose method, but you can access this conveniently with the `T` property.
ski_data[ski_data.Name == ___].___


# It's good that your resort doesn't appear to have any missing values.

# ### 2.6.2 Number Of Missing Values By Column<a id='2.6.2_Number_Of_Missing_Values_By_Column'></a>

# Count the number of missing values in each column and sort them.

# In[ ]:


#Code task 5#
#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of 
#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).
#Order them (increasing or decreasing) using sort_values
#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'
missing = ___([ski_data.___.___, 100 * ski_data.___.___], axis=1)
missing.columns=[___, ___]
missing.___(by=___)


# `fastEight` has the most missing values, at just over 50%. Unfortunately, you see you're also missing quite a few of your desired target quantity, the ticket price, which is missing 15-16% of values. `AdultWeekday` is missing in a few more records than `AdultWeekend`. What overlap is there in these missing values? This is a question you'll want to investigate. You should also point out that `isnull()` is not the only indicator of missing data. Sometimes 'missingness' can be encoded, perhaps by a -1 or 999. Such values are typically chosen because they are "obviously" not genuine values. If you were capturing data on people's heights and weights but missing someone's height, you could certainly encode that as a 0 because no one has a height of zero (in any units). Yet such entries would not be revealed by `isnull()`. Here, you need a data dictionary and/or to spot such values as part of looking for outliers. Someone with a height of zero should definitely show up as an outlier!

# ### 2.6.3 Categorical Features<a id='2.6.3_Categorical_Features'></a>

# So far you've examined only the numeric features. Now you inspect categorical ones such as resort name and state. These are discrete entities. 'Alaska' is a name. Although names can be sorted alphabetically, it makes no sense to take the average of 'Alaska' and 'Arizona'. Similarly, 'Alaska' is before 'Arizona' only lexicographically; it is neither 'less than' nor 'greater than' 'Arizona'. As such, they tend to require different handling than strictly numeric quantities. Note, a feature _can_ be numeric but also categorical. For example, instead of giving the number of `fastEight` lifts, a feature might be `has_fastEights` and have the value 0 or 1 to denote absence or presence of such a lift. In such a case it would not make sense to take an average of this or perform other mathematical calculations on it. Although you digress a little to make a point, month numbers are also, strictly speaking, categorical features. Yes, when a month is represented by its number (1 for January, 2 for Februrary etc.) it provides a convenient way to graph trends over a year. And, arguably, there is some logical interpretation of the average of 1 and 3 (January and March) being 2 (February). However, clearly December of one years precedes January of the next and yet 12 as a number is not less than 1. The numeric quantities in the section above are truly numeric; they are the number of feet in the drop, or acres or years open or the amount of snowfall etc.

# In[ ]:


#Code task 6#
#Use ski_data's `select_dtypes` method to select columns of dtype 'object'
ski_data.___(___)


# You saw earlier on that these three columns had no missing values. But are there any other issues with these columns? Sensible questions to ask here include:
# 
# * Is `Name` (or at least a combination of Name/Region/State) unique?
# * Is `Region` always the same as `state`?

# #### 2.6.3.1 Unique Resort Names<a id='2.6.3.1_Unique_Resort_Names'></a>

# In[ ]:


#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].___.head()


# You have a duplicated resort name: Crystal Mountain.

# **Q: 1** Is this resort duplicated if you take into account Region and/or state as well?

# In[ ]:


#Code task 8#
#Concatenate the string columns 'Name' and 'Region' and count the values again (as above)
(ski_data[___] + ', ' + ski_data[___]).___.head()


# In[ ]:


#Code task 9#
#Concatenate 'Name' and 'state' and count the values again (as above)
(ski_data[___] + ', ' + ski_data[___]).___.head()


# In[ ]:


# **NB** because you know `value_counts()` sorts descending, you can use the `head()` method and know the rest of the counts must be 1.


# **A: 1** Your answer here

# In[11]:


ski_data[ski_data['Name'] == 'Crystal Mountain']


# So there are two Crystal Mountain resorts, but they are clearly two different resorts in two different states. This is a powerful signal that you have unique records on each row.

# #### 2.6.3.2 Region And State<a id='2.6.3.2_Region_And_State'></a>

# What's the relationship between region and state?

# You know they are the same in many cases (e.g. both the Region and the state are given as 'Michigan'). In how many cases do they differ?

# In[ ]:


#Code task 10#
#Calculate the number of times Region does not equal state
(ski_data.Region ___ ski_data.state).___


# You know what a state is. What is a region? You can tabulate the distinct values along with their respective frequencies using `value_counts()`.

# In[13]:


ski_data['Region'].value_counts()


# A casual inspection by eye reveals some non-state names such as Sierra Nevada, Salt Lake City, and Northern California. Tabulate the differences between Region and state. On a note regarding scaling to larger data sets, you might wonder how you could spot such cases when presented with millions of rows. This is an interesting point. Imagine you have access to a database with a Region and state column in a table and there are millions of rows. You wouldn't eyeball all the rows looking for differences! Bear in mind that our first interest lies in establishing the answer to the question "Are they always the same?" One approach might be to ask the database to return records where they differ, but limit the output to 10 rows. If there were differences, you'd only get up to 10 results, and so you wouldn't know whether you'd located all differences, but you'd know that there were 'a nonzero number' of differences. If you got an empty result set back, then you would know that the two columns always had the same value. At the risk of digressing, some values in one column only might be NULL (missing) and different databases treat NULL differently, so be aware that on many an occasion a seamingly 'simple' question gets very interesting to answer very quickly!

# In[ ]:


#Code task 11#
#Filter the ski_data dataframe for rows where 'Region' and 'state' are different,
#group that by 'state' and perform `value_counts` on the 'Region'
(ski_data[ski_data.___ ___ ski_data.___]
 .groupby(___)[___]
 .value_counts())


# The vast majority of the differences are in California, with most Regions being called Sierra Nevada and just one referred to as Northern California.

# #### 2.6.3.3 Number of distinct regions and states<a id='2.6.3.3_Number_of_distinct_regions_and_states'></a>

# In[ ]:


#Code task 12#
#Select the 'Region' and 'state' columns from ski_data and use the `nunique` method to calculate
#the number of unique values in each
ski_data[[___, ___]].___


# Because a few states are split across multiple named regions, there are slightly more unique regions than states.

# #### 2.6.3.4 Distribution Of Resorts By Region And State<a id='2.6.3.4_Distribution_Of_Resorts_By_Region_And_State'></a>

# If this is your first time using [matplotlib](https://matplotlib.org/3.2.2/index.html)'s [subplots](https://matplotlib.org/3.2.2/api/_as_gen/matplotlib.pyplot.subplots.html), you may find the online documentation useful.

# In[ ]:


#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(___, ___, figsize=(___))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind=___, ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title(___)
#Label the xaxis 'Count'
ax[0].set_xlabel(___)
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind=___, ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title(___)
#Label the xaxis 'Count'
ax[1].set_xlabel(___)
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=___);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on


# How's your geography? Looking at the distribution of States, you see New York accounting for the majority of resorts. Our target resort is in Montana, which comes in at 13th place. You should think carefully about how, or whether, you use this information. Does New York command a premium because of its proximity to population? Even if a resort's State were a useful predictor of ticket price, your main interest lies in Montana. Would you want a model that is skewed for accuracy by New York? Should you just filter for Montana and create a Montana-specific model? This would slash your available data volume. Your problem task includes the contextual insight that the data are for resorts all belonging to the same market share. This suggests one might expect prices to be similar amongst them. You can look into this. A boxplot grouped by State is an ideal way to quickly compare prices. Another side note worth bringing up here is that, in reality, the best approach here definitely would include consulting with the client or other domain expert. They might know of good reasons for treating states equivalently or differently. The data scientist is rarely the final arbiter of such a decision. But here, you'll see if we can find any supporting evidence for treating states the same or differently.

# #### 2.6.3.5 Distribution Of Ticket Price By State<a id='2.6.3.5_Distribution_Of_Ticket_Price_By_State'></a>

# Our primary focus is our Big Mountain resort, in Montana. Does the state give you any clues to help decide what your primary target response feature should be (weekend or weekday ticket prices)?

# ##### 2.6.3.5.1 Average weekend and weekday price by state<a id='2.6.3.5.1_Average_weekend_and_weekday_price_by_state'></a>

# In[ ]:


#Code task 14#
# Calculate average weekday and weekend price by state and sort by the average of the two
# Hint: use the pattern dataframe.groupby(<grouping variable>)[<list of columns>].mean()
state_price_means = ski_data.___(___)[[___, ___]].mean()
state_price_means.head()


# In[18]:


# The next bit simply reorders the index by increasing average of weekday and weekend prices
# Compare the index order you get from
# state_price_means.index
# with
# state_price_means.mean(axis=1).sort_values(ascending=False).index
# See how this expression simply sits within the reindex()
(state_price_means.reindex(index=state_price_means.mean(axis=1)
    .sort_values(ascending=False)
    .index)
    .plot(kind='barh', figsize=(10, 10), title='Average ticket price by State'))
plt.xlabel('Price ($)');


# In[ ]:


get_ipython().set_next_input('The figure above represents a dataframe with two columns, one for the average prices of each kind of ticket. This tells you how the average ticket price varies from state to state. But can you get more insight into the difference in the distributions between states');get_ipython().run_line_magic('pinfo', 'states')


# ##### 2.6.3.5.2 Distribution of weekday and weekend price by state<a id='2.6.3.5.2_Distribution_of_weekday_and_weekend_price_by_state'></a>

# Next, you can transform the data into a single column for price with a new categorical column that represents the ticket type.

# In[ ]:


#Code task 15#
#Use the pd.melt function, pass in the ski_data columns 'state', 'AdultWeekday', and 'Adultweekend' only,
#specify 'state' for `id_vars`
#gather the ticket prices from the 'Adultweekday' and 'AdultWeekend' columns using the `value_vars` argument,
#call the resultant price column 'Price' via the `value_name` argument,
#name the weekday/weekend indicator column 'Ticket' via the `var_name` argument
ticket_prices = pd.melt(ski_data[[___, ___, ___]], 
                        id_vars=___, 
                        var_name=___, 
                        value_vars=[___, ___], 
                        value_name=___)


# In[20]:


ticket_prices.head()


# This is now in a format we can pass to [seaborn](https://seaborn.pydata.org/)'s [boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html) function to create boxplots of the ticket price distributions for each ticket type for each state.

# In[ ]:


#Code task 16#
#Create a seaborn boxplot of the ticket price dataframe we created above,
#with 'state' on the x-axis, 'Price' as the y-value, and a hue that indicates 'Ticket'
#This will use boxplot's x, y, hue, and data arguments.
plt.subplots(figsize=(12, 8))
sns.boxplot(x=___, y=___, hue=___, data=ticket_prices)
plt.xticks(rotation='vertical')
plt.ylabel('Price ($)')
plt.xlabel('State');


# Aside from some relatively expensive ticket prices in California, Colorado, and Utah, most prices appear to lie in a broad band from around 25 to over 100 dollars. Some States show more variability than others. Montana and South Dakota, for example, both show fairly small variability as well as matching weekend and weekday ticket prices. Nevada and Utah, on the other hand, show the most range in prices. Some States, notably North Carolina and Virginia, have weekend prices far higher than weekday prices. You could be inspired from this exploration to consider a few potential groupings of resorts, those with low spread, those with lower averages, and those that charge a premium for weekend tickets. However, you're told that you are taking all resorts to be part of the same market share, you  could argue against further segment the resorts. Nevertheless, ways to consider using the State information in your modelling include:
# 
# * disregard State completely
# * retain all State information
# * retain State in the form of Montana vs not Montana, as our target resort is in Montana
# 
# You've also noted another effect above: some States show a marked difference between weekday and weekend ticket prices. It may make sense to allow a model to take into account not just State but also weekend vs weekday.

# Thus we currently have two main questions you want to resolve:
# 
# * What do you do about the two types of ticket price?
# * What do you do about the state information?

# ### 2.6.4 Numeric Features<a id='2.6.4_Numeric_Features'></a>

# In[ ]:


Having decided to reserve judgement on how exactly you utilize the State, turn your attention to cleaning the numeric features.


# #### 2.6.4.1 Numeric data summary<a id='2.6.4.1_Numeric_data_summary'></a>

# In[ ]:


#Code task 17#
#Call ski_data's `describe` method for a statistical summary of the numerical columns
#Hint: there are fewer summary stat columns than features, so displaying the transpose
#will be useful again
ski_data.___.___


# Recall you're missing the ticket prices for some 16% of resorts. This is a fundamental problem that means you simply lack the required data for those resorts and will have to drop those records. But you may have a weekend price and not a weekday price, or vice versa. You want to keep any price you have.

# In[23]:


missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100


# Just over 82% of resorts have no missing ticket price, 3% are missing one value, and 14% are missing both. You will definitely want to drop the records for which you have no price information, however you will not do so just yet. There may still be useful information about the distributions of other features in that 14% of the data.

# #### 2.6.4.2 Distributions Of Feature Values<a id='2.6.4.2_Distributions_Of_Feature_Values'></a>

# Note that, although we are still in the 'data wrangling and cleaning' phase rather than exploratory data analysis, looking at distributions of features is immensely useful in getting a feel for whether the values look sensible and whether there are any obvious outliers to investigate. Some exploratory data analysis belongs here, and data wrangling will inevitably occur later on. It's more a matter of emphasis. Here, we're interesting in focusing on whether distributions look plausible or wrong. Later on, we're more interested in relationships and patterns.

# In[ ]:


#Code task 18#
#Call ski_data's `hist` method to plot histograms of each of the numeric features
#Try passing it an argument figsize=(15,10)
#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing
#It's important you create legible and easy-to-read plots
ski_data.___(___)
#plt.subplots_adjust(hspace=___);
#Hint: notice how the terminating ';' "swallows" some messy output and leads to a tidier notebook


# What features do we have possible cause for concern about and why?
# 
# * SkiableTerrain_ac because values are clustered down the low end,
# * Snow Making_ac for the same reason,
# * fastEight because all but one value is 0 so it has very little variance, and half the values are missing,
# * fastSixes raises an amber flag; it has more variability, but still mostly 0,
# * trams also may get an amber flag for the same reason,
# * yearsOpen because most values are low but it has a maximum of 2019, which strongly suggests someone recorded calendar year rather than number of years.

# ##### 2.6.4.2.1 SkiableTerrain_ac<a id='2.6.4.2.1_SkiableTerrain_ac'></a>

# In[ ]:


#Code task 19#
#Filter the 'SkiableTerrain_ac' column to print the values greater than 10000
ski_data.___[ski_data.___ > ___]


# **Q: 2** One resort has an incredibly large skiable terrain area! Which is it?

# In[ ]:


#Code task 20#
#Now you know there's only one, print the whole row to investigate all values, including seeing the resort name
#Hint: don't forget the transpose will be helpful here
ski_data[ski_data.___ > ___].___


# **A: 2** Your answer here

# But what can you do when you have one record that seems highly suspicious?

# You can see if your data are correct. Search for "silverton mountain skiable area". If you do this, you get some [useful information](https://www.google.com/search?q=silverton+mountain+skiable+area).

# ![Silverton Mountain information](images/silverton_mountain_info.png)

# You can spot check data. You see your top and base elevation values agree, but the skiable area is very different. Your suspect value is 26819, but the value you've just looked up is 1819. The last three digits agree. This sort of error could have occured in transmission or some editing or transcription stage. You could plausibly replace the suspect value with the one you've just obtained. Another cautionary note to make here is that although you're doing this in order to progress with your analysis, this is most definitely an issue that should have been raised and fed back to the client or data originator as a query. You should view this "data correction" step as a means to continue (documenting it carefully as you do in this notebook) rather than an ultimate decision as to what is correct.

# In[ ]:


#Code task 21#
#Use the .loc accessor to print the 'SkiableTerrain_ac' value only for this resort
ski_data.___[39, 'SkiableTerrain_ac']


# In[ ]:


#Code task 22#
#Use the .loc accessor again to modify this value with the correct value of 1819
ski_data.___[39, 'SkiableTerrain_ac'] = ___


# In[ ]:


#Code task 23#
#Use the .loc accessor a final time to verify that the value has been modified
ski_data.___[39, 'SkiableTerrain_ac']


# **NB whilst you may become suspicious about your data quality, and you know you have missing values, you will not here dive down the rabbit hole of checking all values or web scraping to replace missing values.**

# What does the distribution of skiable area look like now?

# In[30]:


ski_data.SkiableTerrain_ac.hist(bins=30)
plt.xlabel('SkiableTerrain_ac')
plt.ylabel('Count')
plt.title('Distribution of skiable area (acres) after replacing erroneous value');


# You now see a rather long tailed distribution. You may wonder about the now most extreme value that is above 8000, but similarly you may also wonder about the value around 7000. If you wanted to spend more time manually checking values you could, but leave this for now. The above distribution is plausible.

# ##### 2.6.4.2.2 Snow Making_ac<a id='2.6.4.2.2_Snow_Making_ac'></a>

# In[31]:


ski_data['Snow Making_ac'][ski_data['Snow Making_ac'] > 1000]


# In[32]:


ski_data[ski_data['Snow Making_ac'] > 3000].T


# You can adopt a similar approach as for the suspect skiable area value and do some spot checking. To save time, here is a link to the website for [Heavenly Mountain Resort](https://www.skiheavenly.com/the-mountain/about-the-mountain/mountain-info.aspx). From this you can glean that you have values for skiable terrain that agree. Furthermore, you can read that snowmaking covers 60% of the trails.

# What, then, is your rough guess for the area covered by snowmaking?

# In[33]:


.6 * 4800


# This is less than the value of 3379 in your data so you may have a judgement call to make. However, notice something else. You have no ticket pricing information at all for this resort. Any further effort spent worrying about values for this resort will be wasted. You'll simply be dropping the entire row!

# ##### 2.6.4.2.3 fastEight<a id='2.6.4.2.3_fastEight'></a>

# Look at the different fastEight values more closely:

# In[34]:


ski_data.fastEight.value_counts()


# Drop the fastEight column in its entirety; half the values are missing and all but the others are the value zero. There is essentially no information in this column.

# In[ ]:


#Code task 24#
#Drop the 'fastEight' column from ski_data. Use inplace=True
ski_data.drop(columns=___, inplace=___)


# What about yearsOpen? How many resorts have purportedly been open for more than 100 years?

# In[ ]:


#Code task 25#
#Filter the 'yearsOpen' column for values greater than 100
ski_data.___[ski_data.___ > ___]


# Okay, one seems to have been open for 104 years. But beyond that, one is down as having been open for 2019 years. This is wrong! What shall you do about this?

# What does the distribution of yearsOpen look like if you exclude just the obviously wrong one?

# In[ ]:


#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.___[ski_data.___ < ___].hist(___)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');


# The above distribution of years seems entirely plausible, including the 104 year value. You can certainly state that no resort will have been open for 2019 years! It likely means the resort opened in 2019. It could also mean the resort is due to open in 2019. You don't know when these data were gathered!

# Let's review the summary statistics for the years under 1000.

# In[38]:


ski_data.yearsOpen[ski_data.yearsOpen < 1000].describe()


# The smallest number of years open otherwise is 6. You can't be sure whether this resort in question has been open zero years or one year and even whether the numbers are projections or actual. In any case, you would be adding a new youngest resort so it feels best to simply drop this row.

# In[39]:


ski_data = ski_data[ski_data.yearsOpen < 1000]


# ##### 2.6.4.2.4 fastSixes and Trams<a id='2.6.4.2.4_fastSixes_and_Trams'></a>

# The other features you had mild concern over, you will not investigate further. Perhaps take some care when using these features.

# ## 2.7 Derive State-wide Summary Statistics For Our Market Segment<a id='2.7_Derive_State-wide_Summary_Statistics_For_Our_Market_Segment'></a>

# You have, by this point removed one row, but it was for a resort that may not have opened yet, or perhaps in its first season. Using your business knowledge, you know that state-wide supply and demand of certain skiing resources may well factor into pricing strategies. Does a resort dominate the available night skiing in a state? Or does it account for a large proportion of the total skiable terrain or days open?
# 
# If you want to add any features to your data that captures the state-wide market size, you should do this now, before dropping any more rows. In the next section, you'll drop rows with missing price information. Although you don't know what those resorts charge for their tickets, you do know the resorts exists and have been open for at least six years. Thus, you'll now calculate some state-wide summary statistics for later use.

# Many features in your data pertain to chairlifts, that is for getting people around each resort. These aren't relevant, nor are the features relating to altitudes. Features that you may be interested in are:
# 
# * TerrainParks
# * SkiableTerrain_ac
# * daysOpenLastYear
# * NightSkiing_ac
# 
# When you think about it, these are features it makes sense to sum: the total number of terrain parks, the total skiable area, the total number of days open, and the total area available for night skiing. You might consider the total number of ski runs, but understand that the skiable area is more informative than just a number of runs.

# A fairly new groupby behaviour is [named aggregation](https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.25.0.html). This allows us to clearly perform the aggregations you want whilst also creating informative output column names.

# In[ ]:


#Code task 27#
#Add named aggregations for the sum of 'daysOpenLastYear', 'TerrainParks', and 'NightSkiing_ac'
#call them 'state_total_days_open', 'state_total_terrain_parks', and 'state_total_nightskiing_ac',
#respectively
#Finally, add a call to the reset_index() method (we recommend you experiment with and without this to see
#what it does)
state_summary = ski_data.groupby('state').agg(
    resorts_per_state=pd.NamedAgg(column='Name', aggfunc='size'), #could pick any column here
    state_total_skiable_area_ac=pd.NamedAgg(column='SkiableTerrain_ac', aggfunc='sum'),
    state_total_days_open=pd.NamedAgg(column=__, aggfunc='sum'),
    ___=pd.NamedAgg(column=___, aggfunc=___),
    ___=pd.NamedAgg(column=___, aggfunc=___)
).___
state_summary.head()


# ## 2.8 Drop Rows With No Price Data<a id='2.8_Drop_Rows_With_No_Price_Data'></a>

# You know there are two columns that refer to price: 'AdultWeekend' and 'AdultWeekday'. You can calculate the number of price values missing per row. This will obviously have to be either 0, 1, or 2, where 0 denotes no price values are missing and 2 denotes that both are missing.

# In[41]:


missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100


# About 14% of the rows have no price data. As the price is your target, these rows are of no use. Time to lose them.

# In[ ]:


#Code task 28#
#Use `missing_price` to remove rows from ski_data where both price values are missing
ski_data = ski_data[___ != 2]


# ## 2.9 Review distributions<a id='2.9_Review_distributions'></a>

# In[43]:


ski_data.hist(figsize=(15, 10))
plt.subplots_adjust(hspace=0.5);


# These distributions are much better. There are clearly some skewed distributions, so keep an eye on `fastQuads`, `fastSixes`, and perhaps `trams`. These lack much variance away from 0 and may have a small number of relatively extreme values.  Models failing to rate a feature as important when domain knowledge tells you it should be is an issue to look out for, as is a model being overly influenced by some extreme values. If you build a good machine learning pipeline, hopefully it will be robust to such issues, but you may also wish to consider nonlinear transformations of features.

# ## 2.10 Population data<a id='2.10_Population_data'></a>

# Population and area data for the US states can be obtained from [wikipedia](https://simple.wikipedia.org/wiki/List_of_U.S._states). Listen, you should have a healthy concern about using data you "found on the Internet". Make sure it comes from a reputable source. This table of data is useful because it allows you to easily pull and incorporate an external data set. It also allows you to proceed with an analysis that includes state sizes and populations for your 'first cut' model. Be explicit about your source (we documented it here in this workflow) and ensure it is open to inspection. All steps are subject to review, and it may be that a client has a specific source of data they trust that you should use to rerun the analysis.

# In[ ]:


#Code task 29#
#Use pandas' `read_html` method to read the table from the URL below
states_url = 'https://simple.wikipedia.org/wiki/List_of_U.S._states'
usa_states = pd.___(___)


# In[45]:


type(usa_states)


# In[46]:


len(usa_states)


# In[47]:


usa_states = usa_states[0]
usa_states.head()


# Note, in even the last year, the capability of `pd.read_html()` has improved. The merged cells you see in the web table are now handled much more conveniently, with 'Phoenix' now being duplicated so the subsequent columns remain aligned. But check this anyway. If you extract the established date column, you should just get dates. Recall previously you used the `.loc` accessor, because you were using labels. Now you want to refer to a column by its index position and so use `.iloc`. For a discussion on the difference use cases of `.loc` and `.iloc` refer to the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html).

# In[ ]:


#Code task 30#
#Use the iloc accessor to get the pandas Series for column number 4 from `usa_states`
#It should be a column of dates
established = usa_sates.___[:, 4]


# In[49]:


established


# Extract the state name, population, and total area (square miles) columns.

# In[ ]:


#Code task 31#
#Now use the iloc accessor again to extract columns 0, 5, and 6 and the dataframe's `copy()` method
#Set the names of these extracted columns to 'state', 'state_population', and 'state_area_sq_miles',
#respectively.
usa_states_sub = usa_states.___[:, [___]].copy()
usa_states_sub.columns = [___]
usa_states_sub.head()


# Do you have all the ski data states accounted for?

# In[ ]:


#Code task 32#
#Find the states in `state_summary` that are not in `usa_states_sub`
#Hint: set(list1) - set(list2) is an easy way to get items in list1 that are not in list2
missing_states = ___(state_summary.state) - ___(usa_states_sub.state)
missing_states


# No?? 

# If you look at the table on the web, you can perhaps start to guess what the problem is. You can confirm your suspicion by pulling out state names that _contain_ 'Massachusetts', 'Pennsylvania', or 'Virginia' from usa_states_sub:

# In[52]:


usa_states_sub.state[usa_states_sub.state.str.contains('Massachusetts|Pennsylvania|Rhode Island|Virginia')]


# Delete square brackets and their contents and try again:

# In[ ]:


#Code task 33#
#Use pandas' Series' `replace()` method to replace anything within square brackets (including the brackets)
#with the empty string. Do this inplace, so you need to specify the arguments:
#to_replace='\[.*\]' #literal square bracket followed by anything or nothing followed by literal closing bracket
#value='' #empty string as replacement
#regex=True #we used a regex in our `to_replace` argument
#inplace=True #Do this "in place"
usa_states_sub.state.___(to_replace=___, value=__, regex=___, inplace=___)
usa_states_sub.state[usa_states_sub.state.str.contains('Massachusetts|Pennsylvania|Rhode Island|Virginia')]


# In[ ]:


#Code task 34#
#And now verify none of our states are missing by checking that there are no states in
#state_summary that are not in usa_states_sub (as earlier using `set()`)
missing_states = ___(state_summary.state) - ___(usa_states_sub.state)
missing_states


# Better! You have an empty set for missing states now. You can confidently add the population and state area columns to the ski resort data.

# In[ ]:


#Code task 35#
#Use 'state_summary's `merge()` method to combine our new data in 'usa_states_sub'
#specify the arguments how='left' and on='state'
state_summary = state_summary.___(usa_states_sub, ___=___, ___=___)
state_summary.head()


# Having created this data frame of summary statistics for various states, it would seem obvious to join this with the ski resort data to augment it with this additional data. You will do this, but not now. In the next notebook you will be exploring the data, including the relationships between the states. For that you want a separate row for each state, as you have here, and joining the data this soon means you'd need to separate and eliminate redundances in the state data when you wanted it.

# ## 2.11 Target Feature<a id='2.11_Target_Feature'></a>

# Finally, what will your target be when modelling ticket price? What relationship is there between weekday and weekend prices?

# In[ ]:


#Code task 36#
#Use ski_data's `plot()` method to create a scatterplot (kind='scatter') with 'AdultWeekday' on the x-axis and
#'AdultWeekend' on the y-axis
ski_data.___(x=___, y=___, kind=___);


# A couple of observations can be made. Firstly, there is a clear line where weekend and weekday prices are equal. Weekend prices being higher than weekday prices seem restricted to sub $100 resorts. Recall from the boxplot earlier that the distribution for weekday and weekend prices in Montana seemed equal. Is this confirmed in the actual data for each resort? Big Mountain resort is in Montana, so the relationship between these quantities in this state are particularly relevant.

# In[ ]:


#Code task 37#
#Use the loc accessor on ski_data to print the 'AdultWeekend' and 'AdultWeekday' columns for Montana only
ski_data.___[ski_data.state == ___, [___, ___]]


# Is there any reason to prefer weekend or weekday prices? Which is missing the least?

# In[58]:


ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum()


# Weekend prices have the least missing values of the two, so drop the weekday prices and then keep just the rows that have weekend price.

# In[59]:


ski_data.drop(columns='AdultWeekday', inplace=True)
ski_data.dropna(subset=['AdultWeekend'], inplace=True)


# In[60]:


ski_data.shape


# Perform a final quick check on the data.

# ### 2.11.1 Number Of Missing Values By Row - Resort<a id='2.11.1_Number_Of_Missing_Values_By_Row_-_Resort'></a>

# Having dropped rows missing the desired target ticket price, what degree of missingness do you have for the remaining rows?

# In[61]:


missing = pd.concat([ski_data.isnull().sum(axis=1), 100 * ski_data.isnull().mean(axis=1)], axis=1)
missing.columns=['count', '%']
missing.sort_values(by='count', ascending=False).head(10)


# These seem possibly curiously quantized...

# In[62]:


missing['%'].unique()


# Yes, the percentage of missing values per row appear in multiples of 4.

# In[63]:


missing['%'].value_counts()


# This is almost as if values have been removed artificially... Nevertheless, what you don't know is how useful the missing features are in predicting ticket price. You shouldn't just drop rows that are missing several useless features.

# In[64]:


ski_data.info()


# There are still some missing values, and it's good to be aware of this, but leave them as is for now.

# ## 2.12 Save data<a id='2.12_Save_data'></a>

# In[65]:


ski_data.shape


# Save this to your data directory, separately. Note that you were provided with the data in `raw_data` and you should saving derived data in a separate location. This guards against overwriting our original data.

# In[66]:


# save the data to a new csv file
datapath = '../data'
save_file(ski_data, 'ski_data_cleaned.csv', datapath)


# In[67]:


# save the state_summary separately.
datapath = '../data'
save_file(state_summary, 'state_summary.csv', datapath)


# ## 2.13 Summary<a id='2.13_Summary'></a>

# **Q: 3** Write a summary statement that highlights the key processes and findings from this notebook. This should include information such as the original number of rows in the data, whether our own resort was actually present etc. What columns, if any, have been removed? Any rows? Summarise the reasons why. Were any other issues found? What remedial actions did you take? State where you are in the project. Can you confirm what the target feature is for your desire to predict ticket price? How many rows were left in the data? Hint: this is a great opportunity to reread your notebook, check all cells have been executed in order and from a "blank slate" (restarting the kernel will do this), and that your workflow makes sense and follows a logical pattern. As you do this you can pull out salient information for inclusion in this summary. Thus, this section will provide an important overview of "what" and "why" without having to dive into the "how" or any unproductive or inconclusive steps along the way.

# **A: 3** Your answer here
